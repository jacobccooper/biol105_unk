---
title: "Descriptive Statistics"
author: "Dr. Jacob C. Cooper"
format: html
editor: visual
---

## Purposes of descriptive statistics

Descriptive statistics enable researchers to quickly and easily examine the "behavior" of their datasets, identifying potential errors and allowing them to observe particular trends that may be worth further analysis. Here, we will cover how to calculate descriptive statistics for multiple different datasets, culminating in an assignment covering these topics.

## Preparing *R*

As with every week, we will need to load our relevant packages first. This week, we are using the following:

```{r}
# allows for internet downloading
library(curl)

# enables data management tools
library(tidyverse)
```

## Downloading the data

For the example this week, we will be using the `starbucks` dataset, describing the number of drinks purchased during particular time periods during the day.

```{r}
starbucks <- curl("https://raw.githubusercontent.com/jacobccooper/biol105_unk/main/datasets/starbucks.csv") %>%
  read_csv()
```

## Descriptive statistics

Descriptive statistics are statistics that help us understand the shape and nature of the data on hand. These include really common metrics such as *mean*, *median*, and *mode*, as well as more nuanced metrics like *quartiles* that help us understand if there is any *skew* in the dataset. (*Skew* refers to a bias in the data, where more data points lie on one side of the distribution and there is a long *tail* of data in the other direction).

![Examples of skew compared to a symmetrical, non-skewed distribution. Source: machinelearningparatodos.com](images/skew.png)

*Note* above that the relative position of the *mean*, *median*, and *mode* can be indicative of skew. Please also note that these values will rarely be exactly equal "in the real world", and thus you need to weigh differences against the entire dataset when assessing skew. There is a lot of nuance like this in statistics; it is not always an "exact" science, but sometimes involves judgment calls and assessments based on what you observe in the data.

Using the `starbucks` dataset, we can look at some of these descriptive statistics to understand what is going on.

### Notation

As a quick reminder, we use Greek lettering for *populations* and Roman lettering for samples. For example:

-   $\sigma$ is a population, but $s$ is a sample (both these variables refer to *standard deviation*).

-   $\mu$ is a population, but $\bar{x}$ is a sample (both of these variables refer to the *mean*).

### Mean

The mean is the "average" value within a set of data, specifically, the sum of all values divided by the length of those values: $\frac{\sum_{i=1}^nx}{n}$.

```{r}
head(starbucks)
```

Here, we are specifically interested in the number of frappuccinos.

```{r}
# get vector of frappuccino number
fraps <- starbucks$Frap_Num

# get mean of vector
mean(fraps)
```

*Note* that the above should be rounded to a whole number, since we were given the data in whole numbers!

```{r}
mean(fraps) %>%
  round(0)
```

We already covered calculating the average manually in our previous tutorial, but we can do that here as well:

```{r}
# sum values
# divide by n, length of vector
# round to 0 places
round(sum(fraps)/length(fraps),0)
```

### Range

The range is the difference between the largest and smallest units in a dataset. We can use the commands `min` and `max` to calculate this.

```{r}
max(fraps) - min(fraps)
```

The range of our dataset is 13.

### Median

The median is also known as the 50th percentile, and is the midpoint of the data when ordered from least to greatest. If there are an even number of data points, then it is the average point between the two center points. For odd data, this is the $\frac{n+1}{2}$th observation. For even data, since we need to take an average, this is the $\frac{\frac{n}{2}+(\frac{n}{2}+1)}{2}$. You should be able to do these by hand and by using a program.

```{r}
median(fraps)
```

Now, to calculate by hand:

```{r}
length(fraps)
```

We have an odd length.

```{r}
# order gets the order
order(fraps)
```

```{r}
# [] tells R which elements to put where
frap_order <- fraps[order(fraps)]

frap_order
```

```{r}
# always use parentheses
# make sure the math maths right!
(length(frap_order)+1)/2
```

Which is the fifth element in the vector?

```{r}
frap_order[5]
```

Now let's try it for an even numbers.

```{r}
# remove first element
even_fraps <- fraps[-1]

even_fraps_order <- even_fraps[order(even_fraps)]

even_fraps_order
```

```{r}
median(even_fraps)
```

Now, by hand: $\frac{\frac{n}{2}+(\frac{n}{2}+1)}{2}$.

```{r}
n <- length(even_fraps_order)

# get n/2 position from vector
m1 <- even_fraps_order[n/2]
# get n/2+1 position
m2 <- even_fraps_order[(n/2)+1]

# add these values, divide by two for "midpoint"
med <- (m1+m2)/2

med
```

As we can see, these values are equal!

### Other quartiles and quantiles

We also use the 25th percentile and the 75th percentile to understand data distributions. These are calculated similar to the above, but the bottom quartile is only $\frac{1}{4}$ of the way between values and the 75th quartile is $\frac{3}{4}$ of the way between values. We can use the *R* function `quantile` to calculate these.

```{r}
quantile(frap_order)
```

We can specify a quantile as well:

```{r}
quantile(frap_order, 0.75)
```

We can also calculate these metrics by hand. Let's do it for the even dataset, since this is more difficult.

```{r}
quantile(even_fraps_order)
```

Note that the 25th and 75th percentiles are also between two different values. These can be calculated as a quarter and three-quarters of the way between their respective values.

```{r}
# 75th percentile

n <- length(even_fraps_order)

# get position
p <- 0.75*(n+1)

# get lower value
# round down
m1 <- even_fraps_order[trunc(p)]

# get upper value
# round up
m2 <- even_fraps_order[ceiling(p)]

# position between
# fractional portion of rank
frac <- p-trunc(p)

# calculate the offset from lowest value
val <- (m2 - m1)*frac

# get value
m1 + val
```

Wait... why does our value differ?

*R*, by default, calculates quantiles using what is called `Type 7`, in which the quantiles are calculated by $p_k = \frac{k-1}{n-1}$, where $n$ is the length of the vector and $k$ refers to the quantile being used. However, in our book and in this class, we use `Type 6` interpretation - $p_k = \frac{k}{n + 1}$. Let's try using `Type 6`:

```{r}
quantile(even_fraps_order, type = 6)
```

Now we have the same answer as we calculated by hand!

This is a classic example of how things in *R* (and in statistics in general!) can depend on interpretation and are not always "hard and fast" rules.

**In this class, we will be using Type 6 interpretation for the quantiles - you will have to specify this in the quantile function EVERY TIME!** If you do *not* specify Type 6, you will get the questions incorrect and you will get answers that do not agree with the book, with Excel, or what you calculate by hand.

### Mode

There is no default method for finding the mode in *R*. However, websites like [Statology](https://www.statology.org/mode-in-r/) provide wraparound functions.

```{r}
# Statology function
# define function to calculate mode
find_mode <- function(x) {
  # get unique values from vector
  u <- unique(x)
  # count number of occurrences for each value
  tab <- tabulate(match(x, u))
  # return the value with the highest count
  u[tab == max(tab)]
}

find_mode(fraps)
```

We can also do this by hand, by counting the number of occurrences of each value. This can be done in a stepwise fashion using commands in the above function.

```{r}
# unique counts
u <- unique(fraps)
u
```

```{r}
# which elements match
match(fraps,u)
```

```{r}
# count them
tab <- match(fraps,u) %>%
  tabulate()

tab
```

Get the highest value.

```{r}
u[tab==max(tab)]
```

Notice this uses `==`. This is a logical argument that means "is equal to" or "is the same as". For example:

```{r}
2 == 2
```

These values are the same, so `TRUE` is returned.

```{r}
2 == 3
```

These values are unequal, so `FALSE` is returned. *R* will read `TRUE` as `1` and `FALSE` as `ZERO`, such that:

```{r}
sum(2==2)
```

and

```{r}
sum(2==3)
```

This allows you to find how many arguments match your condition quickly, and even allows you to subset based on these indices as well. Keep in mind you can use greater than `<`, less than `>`, greater than or equal to `<=`, less than or equal to `>=`, is equal to `==`, and is not equal to `!=` to identify numerical relationships. Other logical arguments include:

-   `&`: both conditions must be `TRUE` to match (e.g., `c(10,20) & c(20,10)`). Try the following as well: `fraps < 10 & fraps > 3`.

-   `&&`: and, but works with single elements and allows for better parsing. Often used with `if`. E.g., `fraps < 10 && fraps > 3`. This will not work on our multi-element `frap` vector.

-   `|`: or, saying at least one condition must be true. Try: `fraps > 10 | fraps < 3`.

-   `||`: or, but for a single element, like `&&` above.

-   `!`: not, so "not equal to" would be `!=`.

### Variance

When we are dealing with datasets, the variance is a measure of the total spread of the data. The variance is calculated using the following:$$\sigma^2=\frac{\sum (x_i-\bar{x})^2}{n-1}$$

Essentially, this means that for every value of $x$, we are finding the difference between that value and the mean and squaring it, summing all of these quared differences, and dividing them by the number of samples in the dataset minus one. Let's do this for the frappuccino dataset.

```{r}
frap_order
```

Now to find the differences.

```{r}
diffs <- frap_order - mean(frap_order)

diffs
```

Note that *R* is calculating the same thing for the entire vector! Since these are differences from the mean, they should sum to zero.

```{r}
sum(diffs)
```

This is not quite zero due to rounding error, but is essentially zero as it is 0.0000000000000036.

```{r}
# square differences
diffs_sq <- diffs^2

diffs_sq
```

Now we have the squared differences. We need to sum these and divide by $n-1$.

```{r}
n <- length(frap_order)

var_frap <- sum(diffs_sq)/(n-1)

var_frap
```

Let's check this against the built-in variance function in *R*.

```{r}
var(frap_order)
```

They are identical! We can check this using a logical argument.

```{r}
var_frap == var(frap_order)
```

Seeing as this is `TRUE`, we calculated it correctly.

### Standard deviation

Another common measurement of spread is the standard deviation ($\sigma$). As you remember from class (or may have guessed from the notation on this site), the standard deviation is just the square root of the variance.

```{r}
sqrt(var_frap)
```

We can test this against the built in `sd` function in *R*:

```{r}
sqrt(var_frap) == sd(frap_order)
```

As you can see, we calculated this correctly!

### Standard error

The standard error is used to help understand the spread of data and to help estimate the accuracy of our measurements for things like the mean. The standard error is calculated thusly:

$$
SE = \frac{\sigma}{\sqrt{n}}
$$

There is not built in function for the standard error in excel, but we can write our own:

```{r}
se <- function(x){
  n <- length(x) # calculate n
  s <- sd(x) # calculate standard deviation
  se_val <- s/sqrt(n)
  return(se_val)
}
```

Let's test this code.

```{r}
se(frap_order)
```

Our code works! And we can see exactly how the standard error is calculate. We can also adjust this code as needed for different situations, like samples.

**Remember**, the standard error is used to help reflect our *confidence* in a specific measurement (*e.g.*, how certain we are of the mean, and what values we believe the mean falls between). We want our estimates to be as precise as possible with as little uncertainty as possible. Given this, does having more samples make our estimates more or less confident? Mathematically, what happens as our sample size *increases*?

### Coefficient of variation

The coefficient of variation, another measure of data spread and location, is calculated by the following:

$$
CV = \frac{\sigma}{\mu}
$$

We can write a function to calculate this in *R* as well.

```{r}
cv <- function(x){
  sigma <- sd(x)
  mu <- mean(x)
  val <- sigma/mu
  return(val)
}

cv(frap_order)
```

*Remember* that we will need to round values.

### Outliers

Outliers are any values that are outside of the 1.5 times the interquartile range. We can calculate this for our example dataset as follows:

```{r}
lowquant <- quantile(frap_order,0.25,type = 6)

hiquant <- quantile(frap_order,0.75,type = 6)

iqr <- hiquant - lowquant

lowbound <- mean(frap_order) - (1.5*iqr)
hibound <- mean(frap_order) + (1.5*iqr)

# low outliers?
# select elements that match
# identify using logical "which"
frap_order[which(frap_order < lowbound)]
```

```{r}
# high outliers?
# select elements that match
# identify using logical "which"
frap_order[which(frap_order > hibound)]
```

We have no outliers for this particular dataset.

## Homework: Chapter 4

Now that we've covered these basic statistics, it's your turn! For this week, you will be completing homework based off of Chapter 4 in your book.

### Homework instructions

Please create an *RMarkdown* document that will render as an `.html` file. You will submit this file to show your coding and your work. Please refer to the [Introduction to *R*](https://jacobccooper.github.io/biol105_unk/intro_to_r.html) for refreshers on how to create an `.html` document in *RMarkdown*. You will need to do the following for each of these datasets:

-   mean

-   median

-   range

-   interquartile range

-   variance

-   standard deviation

-   coefficient of variation

-   standard error

-   whether there are any "outliers"

**Please show all of your work for full credit.**

### Data for homework problems

Please use the following datasets for your homework.

#### Problem 4.1

```{r}
x <- c(2,5,3,7,8,3,9,3,10,4,7,4,6,11,9,
       9,11,5,7,3,8,9,2,1,3,8,3,8,9,3)
```

#### Problem 4.2

```{r}
# beki = Belted Kingfisher (Megaceryle alcyon)
beki <- c(48.1, 50.8, 48.8, 56.8, 57.7, 47.0,
          56.8, 60.2, 55.8, 59.2, 52.5, 50.4,
          48.0, 57.1, 51.8, 52.3, 47.8, 58.0,
          53.4, 55.2, 51.0, 59.3, 61.5, 61.2,
          57.8, 50.1, 56.0, 56.5, 55.8, 56.5,
          56.3, 59.8, 61.8, 56.2, 57.5, 59.3,
          62.4, 61.1, 59.9, 55.6, 56.8, 59.2)
```

#### Problem 4.3

```{r}

```

---
title: "Normality & hypothesis testing"
author: "Dr. Jacob C. Cooper"
format: html
editor: visual
---

## Normal distributions

```{r,echo=F,error=FALSE,warning=FALSE,message=FALSE}
library(curl)
library(tidyverse)
```

A *standard normal distribution* is a mathematical model that describes a commonly observed phenomenon in nature. When measuring many different kinds of datasets, the data being measured often becomes something that resembles a standard normal distribution. This distribution is described by the following equation:

$$f(x)=\frac{1}{\sqrt{2\pi \sigma^2}}e^\frac{(x-\mu)^2}{2\sigma^2}$$

This equation is fairly well defined by the *variance* ($\sigma^2$), the overall spread of the data, and by the *standard deviation* ($\sigma$), which is defined by the square root of the variance.

![A standard normal distribution, illustrating the percentage of area found within each standard deviation away from the mean. By Ainali on Wikipedia; CC-BY-SA 3.0.](images/Standard_deviation_diagram_micro.svg.png)

Standard normal distributions have a mean, median, and mode that are *equal*. The standard normal distribution is a *density function*, and we are interested in the "area under the curve" (AUC) to understand the relative probability of an event occurring. At the mean/median/mode, the probability on either side of the distribution is $50$%. When looking at a normal distribution distribution, it is impossible to say the probability of a specific event occurring, but it is possible to state the probability of an event *as extreme or more extreme than the event observed* occurring. This is known as the $p$ value.

### Example in nature

In order to see an example of the normal distribution in nature, we are going to examine the BeeWalk survey database from the island of Great Britain [@Comont2020]. We are not interested in the bee data at present, however, but in the climatic data from when the surveys were performed.

```{r}
beewalk <- curl("https://figshare.com/ndownloader/files/44726902") %>%
  read_csv()
```

Note that this is another massive dataset - $306,550$ rows of data!

The dataset has the following columns:

```{r}
colnames(beewalk)
```

We are specifically interested in `temperature` to determine weather conditions. Let's see what the mean of this variable is.

```{r}
mean(beewalk$temperature)
```

Hmmm... we are getting an `NA` value, indicating that not every cell has data recorded. Let's view `summary`.

```{r}
summary(beewalk$temperature)
```

As we can see, $16,151$ rows do not have temperature recorded! We want to remove these rows, so we can remove `NA` values using `na.omit`.

```{r}
beewalk$temperature %>%
  na.omit() %>%
  mean() %>%
  round(2) # don't forget to round!
```

Now we can record the mean.

Let's visualize these data using a histogram.

```{r}
hist(beewalk$temperature,breaks = 5)
```

Even with only five breaks, we can see an interesting, normal-esque distribution in the data. Let's refine the bin number.

```{r}
hist(beewalk$temperature,breaks = 40)
```

With forty breaks, the pattern becomes even more clear. Let's see what a *standard normal distribution* around these data would look like.

```{r}
# save temperature vector without NA values
temps <- beewalk$temperature %>% na.omit()

mu <- mean(temps)
t.sd <- sd(temps)

# sample random values
normal.temps <- rnorm(length(temps), # sample same size vector
                      mean = mu,
                      sd = t.sd)

hist(normal.temps, breaks = 40)
```

As we can see, our normal approximation of temperatures is not too dissimilar from the distribution of temperatures we actually see!

Let's see what kind of data we have for temperatures:

```{r}
library(moments)

skewness(temps)
```

Data do not have any significant skew.

```{r}
kurtosis(temps)-3
```

Data do not show any significant kurtosis.

### Effect of sampling

Oftentimes, we will see things approach the normal distribution as we collect more samples. We can model this by subsampling our temperature vector.

```{r}
sub.temps <- sample(temps,
                    size = 10,
                    replace = FALSE)

hist(sub.temps, main = "10 samples")
```

With only ten values sampled, we do not have much of a normal distribution. Let's up this to $100$ samples.

```{r}
sub.temps <- sample(temps,
                    size = 100,
                    replace = FALSE)

hist(sub.temps, main = "100 samples",breaks = 10)
```

Now we are starting to see more of a normal distribution! Let's increase this to $1000$ temperatures.

```{r}
sub.temps <- sample(temps,
                    size = 1000,
                    replace = FALSE)

hist(sub.temps, main = "1000 samples", breaks = 40)
```

Now the normal distribution is even more clear. As we can also see, the more we sample, the more we approach the true means and distribution of the actual dataset. Because of this, we can perform experiments and observations of small groups and subsamples and make inferences about the whole, given that most systems naturally approach statistical distributions like the normal!

## Hypothesis testing

Since we can define specific areas under the curve within these distributions, we can look at the percentage of area within a certain bound to determine how likely a specific outcome would be. Thus, we can begin to test what the *probability of observing an event* is within a theoretical, probabilistic space. A couple of important conceptual ideas:

1.  We may not be able to know the probability of a specific event, but we can figure out the probability of events more extreme or less extreme as that event
2.  If the most likely result is the mean, then the further we move away from the mean, the less likely an event becomes.
3.  If we look *away* from the mean at a certain point, then the area represents the chances of getting a result *as extreme or more extreme than what we observe*. This probability is known as the $p$ value.

Once we have a $p$ value, we can make statements about the event that we've seen relative to the overall nature of the dataset, but we do not have sufficient information to declare if this result is *statistically significant*.

In order to determine if something is significant, we compare things to a *critical value*, known as $\alpha$. This value is traditionally defined as $0.05$, essentially stating that we deem an event as significant if $5$% or fewer of observed or predicted events are as extreme or more extreme than what we observe.

#### Visualizing a $p$ value

Let's say that we are looking at a dataset defined by a standard normal distribution with $\mu=0$ and $\sigma=1$. We draw a random value, $x$, with $x=1.6$. What is the probability of drawing a number this extreme or more extreme from the dataset?

First, let's visualize this distribution:

```{r}
###THIS WILL TAKE A WHILE TO RUN###

# create gigantic normal distribution dataset
# will be essentially normal for plotting
# rnorm gets random values
x <- rnorm(100000000)

# convert to data frame
x <- as.data.frame(x)
# rename column
colnames(x) <- c("values")

# thank you stack overflow for the following
# Creating density plot
p = ggplot(x, 
           aes(x = values)
          ) + 
  # generic density plot, no fill
  geom_density(fill="lightblue")

# Building shaded area
# create new plot object
p2  <-  p + # add previous step as a "backbone"
  # rename axes
  geom_vline(xintercept = 1.6) +
  xlab("Test Statistic") +
  ylab("Frequency (Probability)") +
  # make it neat and tidy
  theme_classic()

# plot it
# can use ggsave function to save
plot(p2)
```

Above, the solid black line represents $x$, with the illustrated standard normal distribution being filled in blue.

Let's see how much of the area represents values *as extreme or more extreme* as our value $x$.

```{r}
### THIS WILL TAKE A WHILE TO RUN ###

# Getting the values of plot
# something I wasn't familiar with before making this!
d  <-  ggplot_build(p)$data[[1]]

# Building shaded area
# create new plot object
p2  <-  p + # add previous step as a "backbone"
  # add new shaded area
  geom_area(data = subset(d, x < 1.6), # select area
            # define color, shading intensity, etc.
            aes(x=x,y=y), fill = "white", alpha = 1) +
  # add value line
  geom_vline(xintercept = 1.6, colour = "black", 
             linetype = "dashed",linewidth = 1) +
  # rename axes
  xlab("Test Statistic") +
  ylab("Frequency (Probability)") +
  # make it neat and tidy
  theme_classic()

# plot it
# can use ggsave function to save
plot(p2)
```

Now we can see that it is only a portion of the distribution that is within our $p$ value, but we don't know exactly what this area is! We can find out in one of two ways.

#### Calculating a $p$ value

We have two different methods for calculating a $p$ value:

##### Comparing to a $z$ table

##### Using *R*

## Homework: Chapter 8
